# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FONCTIONS UTILITAIRES MINIMALES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import csv, io, unicodedata, re, warnings
import pandas as pd
import numpy as np
import streamlit as st

def slugify(txt: str) -> str:
    txt = unicodedata.normalize("NFKD", str(txt)).encode("ascii","ignore").decode()
    return re.sub(r"_+","_", re.sub(r"[^0-9a-z]+","_", txt.lower())).strip("_")

def read_csv_robust(buf) -> pd.DataFrame:
    raw = buf.read()
    sample = raw[:2048].decode("utf-8", "replace")
    try:  sep = csv.Sniffer().sniff(sample, [",", ";", "\t"]).delimiter
    except csv.Error: sep = ","
    buf.seek(0)
    return pd.read_csv(io.BytesIO(raw), sep=sep, dtype=str, keep_default_na=False)

def promote_header(df: pd.DataFrame) -> pd.DataFrame:
    if any(c.lower().startswith("unnamed") for c in df.columns):
        first = df.iloc[0].tolist()
        if len(set(first)) == len(first):
            df = df.iloc[1:].reset_index(drop=True); df.columns = first
    return df

def to_num(s: pd.Series, pct=False):
    s = (s.astype(str)
           .str.replace("%", "", regex=False)
           .str.replace(",", "", regex=False)
           .str.replace(" ", "", regex=False)   # espace ins√©cable
           .str.replace(" ", "", regex=False))
    n = pd.to_numeric(s, errors="coerce")
    return n/100 if pct else n

def clean(df0: pd.DataFrame) -> pd.DataFrame:
    df = promote_header(df0); df.dropna(how="all", inplace=True)
    df.columns = [slugify(c) or f"col_{i}" for i, c in enumerate(df.columns)]
    for col, pct in {"volumetrie_an": False, "gains_etp": False, "reussite": True}.items():
        if col in df.columns: df[col] = to_num(df[col], pct)
    return df
# -----------------------------------------------------------

st.sidebar.header("üìÇ Import")
upl = st.sidebar.file_uploader("CSV UTF-8", ["csv"])
if upl is None:
    st.sidebar.info("‚û°Ô∏è D√©posez un fichier pour commencer."); st.stop()

df = clean(read_csv_robust(upl))
df_original = df.copy()          # <-- ta ligne 27 redeviendra valide


# Biblioth√®ques pour les visualisations avanc√©es
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Titre de l'application
st.title("Dashboard Inventaire RPA - Analyses Avanc√©es")

# Chargement des donn√©es (√† adapter selon la source r√©elle des donn√©es)
# Ici on suppose un fichier CSV 'inventaire_rpa.csv' disponible, sinon utiliser st.file_uploader
# df = pd.read_csv('inventaire_rpa.csv')
# Pour l'exemple, on part du principe que df est d√©j√† fourni dans l'app v3.0 existante.

# --- Pr√©paration des donn√©es ---
# Copie de sauvegarde
df_original = df.copy()
# Optionnel: Renommer certaines colonnes pour simplifier la manipulation (pas obligatoire si on utilise les noms directement)
df = df.rename(columns={
    'Zone fonctionnelle': 'Zone',
    'D√©partement': 'Departement',
    'Techno': 'Technologie',
    'Vendor': 'Vendor',
    'Cloud/On premise': 'Environnement',
    'Volum√©trie/an': 'Volumetrie_an',
    '%R√©ussite': 'Taux_reussite',
    'Gains ETP': 'Gains_ETP'
})

# Encodage de certaines colonnes cat√©gorielles utiles pour analyses (sans modifier le df original)
# (Par exemple pour PCA/KMeans on peut vouloir des colonnes num√©riques)
encoded_df = df.copy()
# On peut encoder 'Complexit√©' si elle est cat√©gorielle (faible/moyenne/haute) en valeurs num√©riques
if 'Complexit√©' in encoded_df.columns:
    # Mapping manuel possible si Complexit√© est textual, par ex:
    complexity_map = {'Faible': 1, 'Moyenne': 2, 'Haute': 3}
    encoded_df['Complexite_num'] = encoded_df['Complexit√©'].map(complexity_map)
# Encodage des autres cat√©gories avec LabelEncoder si besoin
for col in ['Zone', 'Departement', 'Technologie', 'Vendor', 'Environnement', 'Synch/Asynch']:
    if col in encoded_df.columns:
        le = LabelEncoder()
        encoded_df[col] = le.fit_transform(encoded_df[col].astype(str))

# --- Cr√©ation des onglets pour organiser les visualisations ---
tab1, tab2, tab3, tab4 = st.tabs(["Statistiques Descriptives", "Clustering & R√©duction de Dimension", "Mod√©lisation Pr√©dictive", "Visuels de Synth√®se"])

# ========== Onglet 1: Statistiques Descriptives ==========
with tab1:
    st.header("Analyse Statistique Descriptive")
    # Sous-section: Statistiques globales
    st.subheader("Statistiques globales")
    st.write("**Nombre total de processus RPA :** ", len(df))
    # Afficher quelques indicateurs cl√©s agr√©g√©s (somme des gains, moyenne taux r√©ussite, etc.)
    if 'Gains_ETP' in df.columns:
        total_gains = df['Gains_ETP'].sum()
        st.write(f"**Gains ETP total (somme) :** {total_gains:.2f}")
    if 'Taux_reussite' in df.columns:
        avg_success = df['Taux_reussite'].mean()
        st.write(f"**Taux de r√©ussite moyen :** {avg_success:.1f}%")
    # etc. (on peut ajouter d'autres KPIs globaux si pertinent)

    # Sous-section: Distribution des variables num√©riques (Histogrammes)
    st.subheader("Distribution des variables num√©riques")
    numeric_cols = ['Volumetrie_an', 'Taux_reussite', 'Gains_ETP']
    for col in numeric_cols:
        if col in df.columns:
            fig, ax = plt.subplots()
            sns.histplot(df[col].dropna(), kde=True, ax=ax)  # histogramme avec courbe de densit√©
            ax.set_title(f"Histogramme de {col}")
            st.pyplot(fig)

    # Sous-section: Boxplots par cat√©gorie (ex: Gains ETP par Complexit√©)
    if 'Gains_ETP' in df.columns and 'Complexit√©' in df.columns:
        st.subheader("R√©partition des gains ETP par niveau de complexit√©")
        fig, ax = plt.subplots()
        sns.boxplot(x=df['Complexit√©'], y=df['Gains_ETP'], ax=ax)
        ax.set_title("Gains ETP par niveau de Complexit√©")
        st.pyplot(fig)

    # Sous-section: Bar charts des distributions cat√©gorielles
    st.subheader("Nombre de processus par cat√©gorie")
    col1, col2 = st.columns(2)
    # Bar chart par Zone fonctionnelle
    if 'Zone' in df.columns:
        counts = df['Zone'].value_counts()
        fig1 = px.bar(x=counts.index, y=counts.values, labels={'x': 'Zone fonctionnelle', 'y': 'Nombre de processus'}, title="Processus par Zone fonctionnelle")
        col1.plotly_chart(fig1, use_container_width=True)
    # Bar chart par D√©partement
    if 'Departement' in df.columns:
        counts2 = df['Departement'].value_counts()
        fig2 = px.bar(x=counts2.index, y=counts2.values, labels={'x': 'D√©partement', 'y': 'Nombre de processus'}, title="Processus par D√©partement")
        col2.plotly_chart(fig2, use_container_width=True)

    # Sous-section: Matrice de corr√©lation (heatmap)
    st.subheader("Matrice de corr√©lation")
    # Calcul de la matrice de corr√©lation sur les variables num√©riques principales
    corr_matrix = df[['Volumetrie_an', 'Taux_reussite', 'Gains_ETP']].corr() if all(x in df.columns for x in ['Volumetrie_an','Taux_reussite','Gains_ETP']) else None
    if corr_matrix is not None:
        fig, ax = plt.subplots()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax)
        ax.set_title("Heatmap de corr√©lation")
        st.pyplot(fig)
    else:
        st.write("Pas de corr√©lation calculable (colonnes manquantes).")

    # Sous-section: Scatter plot Gains vs Volumetrie avec tendance
    if 'Volumetrie_an' in df.columns and 'Gains_ETP' in df.columns:
        st.subheader("Relation Volume vs Gains (avec tendance)")
        fig = px.scatter(df, x='Volumetrie_an', y='Gains_ETP', trendline="ols",
                         labels={'Volumetrie_an': 'Volum√©trie/an', 'Gains_ETP': 'Gains ETP'},
                         title="Gains ETP en fonction de la Volum√©trie annuelle")
        st.plotly_chart(fig, use_container_width=True)

# ========== Onglet 2: Clustering & R√©duction de Dimension ==========
with tab2:
    st.header("Clustering et R√©duction de Dimension")
    # Pr√©paration des donn√©es num√©riques pour PCA/Clustering
    features_for_analysis = []
    for col in ['Volumetrie_an','Taux_reussite','Gains_ETP','Complexite_num']:
        if col in encoded_df.columns:
            features_for_analysis.append(col)
    data_for_analysis = encoded_df[features_for_analysis].dropna()
    # Standardisation des donn√©es
    data_scaled = StandardScaler().fit_transform(data_for_analysis.values) if len(data_for_analysis)>0 else None

    # Application du PCA √† 2 composantes
    if data_scaled is not None and data_scaled.shape[1] >= 2:
        pca = PCA(n_components=2)
        proj = pca.fit_transform(data_scaled)
        df_pca = pd.DataFrame(proj, columns=['PC1','PC2'])
        # Ajout de labels pour la couleur (on choisit une variable cat√©gorielle existante si possible)
        if 'Zone' in df.columns:
            df_pca['Couleur'] = df.loc[data_for_analysis.index, 'Zone']
        elif 'Technologie' in df.columns:
            df_pca['Couleur'] = df.loc[data_for_analysis.index, 'Technologie']
        else:
            df_pca['Couleur'] = None
        st.subheader("Projection PCA (2 composantes principales)")
        fig = px.scatter(df_pca, x='PC1', y='PC2', color='Couleur', title="Projection PCA des processus RPA")
        st.plotly_chart(fig, use_container_width=True)
        # Afficher la variance expliqu√©e
        exp_var = pca.explained_variance_ratio_
        st.caption(f"Variance expliqu√©e par PC1 et PC2 : {exp_var[0]*100:.1f}% + {exp_var[1]*100:.1f}% = {sum(exp_var)*100:.1f}%")

    # Application de t-SNE pour visualisation non-lin√©aire (2D)
    if data_scaled is not None:
        tsne = TSNE(n_components=2, random_state=42, perplexity=15, n_iter=500)
        proj_tsne = tsne.fit_transform(data_scaled)
        df_tsne = pd.DataFrame(proj_tsne, columns=['TSNE1','TSNE2'])
        if 'Zone' in df.columns:
            df_tsne['Couleur'] = df.loc[data_for_analysis.index, 'Zone']
        elif 'Technologie' in df.columns:
            df_tsne['Couleur'] = df.loc[data_for_analysis.index, 'Technologie']
        else:
            df_tsne['Couleur'] = None
        st.subheader("Projection t-SNE (2 composantes)")
        fig = px.scatter(df_tsne, x='TSNE1', y='TSNE2', color='Couleur', title="t-SNE des processus RPA")
        st.plotly_chart(fig, use_container_width=True)

    # Clustering K-Means sur les m√™mes features
    if data_scaled is not None:
        # Choix du nombre de clusters k (exemple k=3)
        k = 3
        kmeans = KMeans(n_clusters=k, random_state=1, n_init='auto')
        labels = kmeans.fit_predict(data_scaled)
        df_clust = df.loc[data_for_analysis.index].copy()
        df_clust['Cluster'] = labels
        # Info sur les clusters
        st.subheader("Clustering K-Means")
        st.write(f"**Nombre de clusters choisi :** {k}")
        # Afficher taille et caract√©ristiques moyennes de chaque cluster
        cluster_summary = df_clust.groupby('Cluster')[['Volumetrie_an','Taux_reussite','Gains_ETP']].mean()
        cluster_counts = df_clust['Cluster'].value_counts()
        st.write("**Effectifs par cluster :**", cluster_counts.to_dict())
        st.write("**Moyennes des variables par cluster :**")
        st.dataframe(cluster_summary)
        # Visualisation des clusters sur PCA (si calcul√©)
        if data_scaled.shape[1] >= 2:
            df_pca['Cluster'] = labels
            fig = px.scatter(df_pca, x='PC1', y='PC2', color=df_pca['Cluster'].astype(str), title="Clusters (K-Means) sur projection PCA")
            st.plotly_chart(fig, use_container_width=True)

    # D√©tection d'outliers simples via Z-score
    st.subheader("Processus RPA atypiques (Outliers)")
    if data_scaled is not None:
        # Calcul du Z-score moyen (somme des valeurs absolues des z-scores sur chaque variable normalis√©e)
        z_scores = np.abs((data_for_analysis - data_for_analysis.mean())/data_for_analysis.std(ddof=0))
        z_sum = z_scores.sum(axis=1)
        # On consid√®re comme outliers les 5 plus grands scores
        outlier_idx = z_sum.nlargest(5).index
        outliers = df.loc[outlier_idx, ['Id','Nom','Zone','Departement','Volumetrie_an','Taux_reussite','Gains_ETP']]
        st.write("Top 5 des processus les plus atypiques (selon un score d'anomalie simple) :")
        st.dataframe(outliers)
    else:
        st.write("Donn√©es insuffisantes pour calculer des outliers.")

# ========== Onglet 3: Mod√©lisation Pr√©dictive ==========
with tab3:
    st.header("Mod√©lisation Pr√©dictive & Importance des Variables")
    # R√©gression lin√©aire simple (Volumetrie -> Gains)
    st.subheader("R√©gression lin√©aire : Gains ETP ~ Volum√©trie/an")
    if 'Volumetrie_an' in df.columns and 'Gains_ETP' in df.columns:
        # On utilise Plotly Express avec trendline pour illustrer la r√©gression
        fig = px.scatter(df, x='Volumetrie_an', y='Gains_ETP', trendline="ols",
                         labels={'Volumetrie_an':'Volum√©trie/an', 'Gains_ETP':'Gains ETP'},
                         title="Relation entre volum√©trie et gains ETP (r√©gression OLS)")
        st.plotly_chart(fig, use_container_width=True)
        # On peut extraire les param√®tres de la r√©gression via les r√©sultats statsmodels si besoin:
        # results = px.get_trendline_results(fig)
        # st.write(results.px_fit_results.iloc[0].summary())
    else:
        st.write("Variables Volum√©trie/an ou Gains ETP manquantes, r√©gression non effectu√©e.")

    # Importance des variables via Random Forest
    st.subheader("Importance des variables (Random Forest)")
    # Exemple: pr√©dire un succ√®s √©lev√©/faible en classification (on d√©finit succ√®s √©lev√© si % r√©ussite > 90% par ex)
    if 'Taux_reussite' in df.columns:
        # Cr√©er une classe binaire succ√®s (1 si taux >= 90, 0 sinon) pour l'exemple
        df_model = encoded_df.dropna(subset=['Taux_reussite']).copy()
        df_model['HighSuccess'] = (df_model['Taux_reussite'] >= 90).astype(int)
        # Features candidates (on √©vite d'inclure Taux_reussite √©videmment)
        feature_candidates = ['Volumetrie_an','Complexite_num','Gains_ETP']
        feature_cols = [col for col in feature_candidates if col in df_model.columns]
        X = df_model[feature_cols]
        y = df_model['HighSuccess']
        if len(X) > 0:
            # Entra√Ænement d'un RandomForestClassifier simple
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            model.fit(X, y)
            importances = model.feature_importances_
            feat_imp = pd.Series(importances, index=feature_cols).sort_values(ascending=False)
            fig = px.bar(x=feat_imp.values, y=feat_imp.index, orientation='h',
                         labels={'x':'Importance', 'y':'Variable'}, title="Importance des variables - Random Forest")
            st.plotly_chart(fig, use_container_width=True)
            st.caption("Les variables les plus importantes pour pr√©dire un haut taux de r√©ussite.")
        else:
            st.write("Pas de donn√©es suffisantes pour calculer les importances.")
    else:
        st.write("Taux de r√©ussite non disponible, importance des variables non calcul√©e.")

# ========== Onglet 4: Visuels de Synth√®se (PPT) ==========
with tab4:
    st.header("Graphiques de Synth√®se (Style PPT)")
    # Radar Chart: Profil par Zone fonctionnelle (moyennes normalis√©es des indicateurs)
    if 'Zone' in df.columns:
        st.subheader("Profil par Zone (Radar Chart)")
        radar_metrics = ['Volumetrie_an','Taux_reussite','Gains_ETP']
        # Calcul des moyennes par zone
        radar_data = df.groupby('Zone')[radar_metrics].mean().reset_index()
        # Normaliser chaque m√©trique 0-1 pour comparer les formes (optionnel)
        radar_norm = radar_data.copy()
        for m in radar_metrics:
            max_val = radar_data[m].max()
            min_val = radar_data[m].min()
            if max_val > min_val:
                radar_norm[m] = (radar_data[m] - min_val) / (max_val - min_val)
            else:
                radar_norm[m] = 0.0
        # Pr√©parer les donn√©es pour plotly (trace par zone)
        fig = go.Figure()
        categories = radar_metrics
        for _, row in radar_norm.iterrows():
            zone = row['Zone']
            values = [row[m] for m in radar_metrics]
            # boucler pour fermer le polygone
            values += [values[0]]
            fig.add_trace(go.Scatterpolar(r=values, theta=categories+ [categories[0]], fill='toself', name=str(zone)))
        fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0,1])), title="Profil comparatif des zones (moyennes normalis√©es)")
        st.plotly_chart(fig, use_container_width=True)

    # Waterfall Chart: Contribution des d√©partements aux gains totaux
    if 'Gains_ETP' in df.columns and 'Departement' in df.columns:
        st.subheader("Contribution des D√©partements aux Gains Totaux (Waterfall)")
        dept_sum = df.groupby('Departement')['Gains_ETP'].sum().reset_index()
        dept_sum = dept_sum.sort_values('Gains_ETP', ascending=False)
        # Pr√©parer le waterfall via plotly
        fig = go.Figure(go.Waterfall(
            name="Gains", orientation="v",
            x=dept_sum['Departement'],
            y=dept_sum['Gains_ETP'],
            text=[f"{val:.1f}" for val in dept_sum['Gains_ETP']],
            textposition="outside"
        ))
        fig.update_layout(title="Gains ETP par d√©partement - Waterfall", showlegend=False)
        st.plotly_chart(fig, use_container_width=True)

    # Word Cloud des descriptions
    st.subheader("Nuage de Mots des Descriptions")
    if 'Description' in df.columns:
        text = " ".join(df['Description'].astype(str).tolist())
        if text.strip():
            wordcloud = WordCloud(width=800, height=400, background_color="white", max_words=100, collocations=False).generate(text)
            fig, ax = plt.subplots(figsize=(6,3))
            ax.imshow(wordcloud, interpolation='bilinear')
            ax.axis("off")
            st.pyplot(fig)
        else:
            st.write("Pas de descriptions disponibles pour g√©n√©rer un word cloud.")
    else:
        st.write("Champ 'Description' non disponible.")

    # Sunburst Chart: R√©partition hi√©rarchique Zone -> D√©partement
    if 'Zone' in df.columns and 'Departement' in df.columns:
        st.subheader("R√©partition hi√©rarchique Zone / D√©partement (Sunburst)")
        fig = px.sunburst(df, path=['Zone','Departement'], values=None, title="Nombre de processus par Zone et D√©partement")
        st.plotly_chart(fig, use_container_width=True)

    # Sankey Diagram: Flux Zone -> Technologie (ou Vendor)
    if 'Zone' in df.columns and 'Vendor' in df.columns:
        st.subheader("Utilisation des Technologies par Zone (Sankey)")
        # Construire les noeuds et liens
        zones = list(df['Zone'].unique())
        vendors = list(df['Vendor'].unique())
        all_nodes = zones + vendors
        # dictionnaires pour index
        idx = {node: i for i, node in enumerate(all_nodes)}
        # Calculer les liens (nombre de processus par zone-vendor)
        links = df.groupby(['Zone','Vendor'])['Id'].count().reset_index()
        # Cr√©er les listes source, target, value pour le sankey
        sources = links['Zone'].map(idx).tolist()
        targets = links['Vendor'].map(idx).tolist()
        values = links['Id'].tolist()
        # Diagramme Sankey
        fig = go.Figure(data=[go.Sankey(
            node=dict(label=all_nodes, pad=15, thickness=20),
            link=dict(source=sources, target=targets, value=values)
        )])
        fig.update_layout(title_text="R√©partition des technologies RPA par zone", font_size=10)
        st.plotly_chart(fig, use_container_width=True)
